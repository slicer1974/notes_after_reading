Новые нереляционные хранилища данных NoSQL делятся на две основные разновидности. 
1. Документоориентированные БД предназначены для тех сценариев использо- вания, при которых данные поступают в виде отдельных документов и связи между документами редки.
2. Графовые БД нацелены в противоположном направлении: они предназначены для сценариев применения, в которых любые данные потенциально могут быть взаимосвязаны.
У каждой модели данных есть собственный язык запросов или фреймворк, несколько их примеров: SQL, MapReduce, конвейер агрегирования MongoDB, Cypher, SPARQL и Datalog.

Два семейства подсистем хранения: журналированные (log-structured storage engine) и постраничные (page-oriented storage engine), например B-деревья.
Многие БД используют механизм, __журнал__, представляющий собой файл, предназначенный только для добавления данных в его конец. 

#Самый простой вариант: ключи в хеш-таблицах, дописывание новых записей в конец, для устранения проблемы исчерпания места при росте размера таблицы используется уплотнение (отбрасывание дублирующихся ключей из журнала и сохранение только последней версии данных для каждого ключа). 
__Плюсы__:
Добавление в конец файла и слияние сегментов — последовательные операции записи, в большинстве случаев выполняющиеся намного быстрее случайной за- писи, особенно на магнитных жестких дисках с раскручивающимися пластина- ми. Последовательная запись до некоторой степени предпочтительна и в случае твердотельных накопителей (solid state drive, SSD) на основе флеш-памяти. 
Конкурентный доступ и восстановление после сбоев сильно упрощаются в слу- чае допускающих только добавление или вообще неизменяемых файлов дан- ных. Например, не нужно заботиться о сбоях во время перезаписи значения, приводящих в результате к файлу, в котором склеены воедино части старого и нового значений. 
Слияние старых сегментов позволяет решить проблему фрагментирования файлов данных с течением времени. 
__Минусы__:
Хеш-таблица должна помещаться в оперативной памяти, так что если у вас очень много ключей, то вам не повезло. В принципе, можно поддерживать хеш-карту на диске, но, к сожалению, добиться хорошей ее производительности непросто. Она требует большого количества операций ввода/вывода с произ- вольным доступом, ее расширение при заполнении — дорогостоящая операция, а разрешение конфликтов хеша требует запутанной логики.
Запросы по диапазону неэффективны. Например, невозможно с легкостью просмотреть все записи между kitty00000 и kitty99999 — необходимо искать каждый ключ отдельно в хеш-картах.

#Более удобный вариант - поменять формат наших файлов сегментов: потребовать, чтобы последовательность пар «ключ — значение» была отсортирована по ключу. 
__Плюсы__:
1. Объединение сегментов выполняется просто и эффективно, даже если размер файлов превышает объем доступной оперативной памяти. Этот подход близок к используемому в алгоритме сортировки слиянием (mergesort). 
2. Чтобы найти в файле конкретный ключ, не нужно больше хранить индекс всех ключей в оперативной памяти. Можно двигаться от ближайшего известного индекса последовательным перебором.
3. Поскольку для выполнения запроса на чтение все равно необходимо просмо- треть несколько пар «ключ — значение» из заданного диапазона, вполне можно сгруппировать эти записи в блок и сжать его перед записью на диск. Каждая запись разреженного индекса в опе- ративной памяти затем будет указывать на начало сжатого блока. Помимо экономии пространства на диске, сжатие также снижает использование полосы пропускания ввода/вывода.

#Организация работы подсистемы хранения на основе журналированного дерева слияния (Log-Structured Merge-Tree, LSM-Tree):
При поступлении записи добавляем ее в располагающуюся в оперативной па- мяти сбалансированную структуру данных (например, красно-черное дерево). Это располагающееся в оперативной памяти дерево называется MemTable (от memory table — «таблица, расположенная в памяти»). 
Когда размер MemTable превышает определенное пороговое значение — обычно несколько мегабайт, — записываем его на диск в виде файла __sorted strings table (SS-таблицы)__. Эта операция выполняется достаточно эффективно, поскольку дерево поддер- живает пары «ключ — значение» в отсортированном по ключу виде. Новый файл SS-таблицы становится последним сегментом базы данных. А пока SS-таблица записывается на диск, операции записи продолжают выполняться в новый экземпляр MemTable. 
Для обслуживания запроса на чтение сначала пробуем найти ключ в MemTable, затем в последнем по времени сегменте на диске, затем в предпоследнем и т. д.
Время от времени запускаем в фоне процесс слияния и уплотнения, чтобы объединить файлы сегментов и отбросить перезаписанные или удаленные значения. 
Представленная схема работает отлично. У нее есть только одна проблема: если происходит фатальный сбой БД, то записанные позже всего данные (находящиеся в MemTable, но еще не записанные на диск) теряются. Чтобы избежать этой проблемы, можно держать на диске отдельный журнал, в конец которого немедленно добавляются все записываемые данные, точно так же, как в предыдущем разделе. Сам журнал неупорядочен, но это неважно, ведь его единственное назначение — восстановление MemTable после сбоя. Всякий раз, когда MemTable записывается в SS-таблицу, соответствующий журнал можно удалять.
__Минусы:__
Может работать медленно при поиске отсутствующих в базе данных ключей. Чтобы оптимизировать подобную разновидность доступа, подсистемы хранения часто применяют дополнительные фильтры Блума. (Фильтр Блума — это эффективно использующая память структура данных для приближенного определения содержимого множества. Она позволяет установить, встречается ли ключ в базе данных, экономя таким образом множество лишних чтений диска для несуществующих ключей.)

#B-деревья 
очень хорошо выдержали испытание временем. Они остаются стандартной реализацией индексов практически во всех реляционных базах данных, да и многие нереляционные тоже их применяют. Аналогично SS-таблицам B-деревья хранят пары «ключ — значение» в отсортиро- ванном по ключу виде, что позволяет эффективно выполнять поиск значения по ключу и запросы по диапазонам.
__Журналированные индексы__, которые мы видели ранее, разбивают базу данных на __сегменты переменного размера__, обычно несколько мегабайт или более, и всегда за- писывают их на диск последовательно. В отличие от них, __B-деревья__ разбивают БД на __блоки или страницы фиксированного размера__, обычно 4 Кбайт (иногда больше), и читают/записывают по одной странице за раз. Такая конструкция лучше подходит для нижележащего аппаратного обеспечения, поскольку диски тоже разбиваются на блоки фиксированного размера. Все страницы имеют свой адрес/местоположение, благодаря чему одни страницы могут ссылаться на другие — аналогично указателям, но на диске, а не в памяти. Этими ссылками на страницы можно воспользоваться для создания дерева страниц.
Одна из страниц назначается __корнем B-дерева__, с него начинается любой поиск ключа в индексе. Данная страница содержит несколько ключей и ссылок на до- черние страницы. Каждая из них отвечает за непрерывный диапазон ключей, а ключи, располагающиеся между ссылками, указывают на расположение границ этих диапазонов.
Количество ссылок на дочерние страницы на одной странице B-дерева называется __коэффициентом ветвления (branching factor)__.
Базовая операция записи __B-дерева — перезапись страницы__ на диске новыми дан- ными. Предполагается, что эта перезапись не меняет расположения страницы, то есть все ссылки на нее остаются неизменными. Это резко отличается от журнализированных индексов, например, __LSM-деревьев__, в которых происходит __только дописывание__ (и постепенное удаление устаревших файлов), но __не изменение__ существующих файлов.
Чтобы сделать БД __отказоустойчивой__, реализации B-деревьев обычно включают дополнительную структуру данных на диске: __журнал упреждающей записи__ (write- ahead log, WAL), также именуемый __журналом повтора__ (redo log). Он представляет собой файл, предназначенный только для добавления, в который все модификации B-деревьев должны записываться еще до того, как применяться к самим страницам дерева. Когда база возвращается в норму после сбоя, этот журнал используется для восстановления B-дерева в согласованное состояние.

Запросы ищут в индексе ключ, а значение между тем может быть __фактической искомой строкой__ (документом, вершиной) или __ссылкой на строку__, хранящуюся где-то в другом месте. Во втором случае место, где хранятся строки, называется неупорядоченным файлом (heap file), и данные там хранятся, соответственно, в неупорядоченном виде. (Это может быть файл, предназначенный только для добавления данных, или же в нем могут отслеживаться удаленные строки, чтобы позднее перезаписать их новыми данными.) Неупорядоченные файлы использу- ются весьма часто, ведь они позволяют избежать дублирования данных в случае нескольких вторичных индексов: все индексы лишь ссылаются на местоположение в неупорядоченном файле, а фактические данные хранятся в одном месте.
В некоторых случаях лишний переход от индекса к неупорядоченному файлу — слишком затратная вещь для чтения в смысле производительности, поэтому желательно хранить проиндексированную строку непосредственно в индексе. Такой вариант носит название __кластеризованного индекса (clustered index)__. Например, в подсистеме хранения InnoDB СУБД MySQL первичный ключ таблицы всегда представляет собой кластеризованный индекс, а вторичные индексы ссылаются на первичный ключ (а не на позицию в неупорядоченном файле).
Компромисс между кластеризованным индексом (хранением всех ключей в ин- дексе) и некластеризованным (хранением в индексе только ссылок на данные) известен под названием __охватывающего индекса (covering index)__ или индекса с включенными столбцами (index with included columns). При этом в индексе хра- нится только часть столбцов таблицы.
Наиболее распространенный тип составных индексов — __сцепленный индекс (concatenated index)__, который просто объединяет несколько полей в один ключ, присоединяя один столбец к другому (в описании индекса указывается, в каком порядке сцепляются поля).
Многомерные индексы — более общий способ запроса нескольких столбцов сразу, особенно важный при работе с геопространственными данными. Один из вариантов решения  проблемы поиска в сложной таблице по долготе и широте — преобразовать двумерное место- положение в одно число с помощью заполняющей пространство кривой, после чего воспользоваться обычным индексом на основе B-дерева. Но чаще применяются специализированные пространственные индексы, такие как R-деревья. Напри- мер, в PostGIS геопространственные индексы реализуются в виде R-деревьев благодаря функции Generalized Search Tree (обобщенное дерево поиска) СУБД PostgreSQL.
Системы полнотекстового поиска зачастую оперируют __нечетким (fuzzy)__ поиском. Например, системы полнотекстового поиска обычно позволяют расширять поиск по слову путем включения синонимов этого слова, игнорирования грамматических вариантов слов, поиска вхождения слов рядом друг с другом в одном документе и поддерживают другие возможности, основанные на лингвистическом анализе текста.
По мере удешевления RAM аргумент относительно стоимости в пересчете на гигабайт памяти постепенно слабеет. Многие наборы данных попросту не настолько велики, так что вполне допустимо держать их полностью в оперативной памяти, возможно распределенной по нескольким машинам. Этот факт привел к разработке __размещаемых в оперативной памяти БД (in-memory databases).__
Помимо производительности, еще одно интересное преимущество располага­емых в памяти БД состоит в предоставлении моделей данных, которые сложно реализовать с помощью дисковых индексов. Например, Redis обеспечивает напоминающий БД интерфейс для различных структур данных, таких как очереди по приоритету и множества. В силу хранения всей информации в оперативной памяти его реализация довольно проста.

#OLAP и OLTP
__Склад данных (data warehouse)__, представляет собой отдельную БД, ко- торую аналитики могут опрашивать так, как им заблагорассудится, не влияя при этом на OLTP-операции. Склад содержит предназначенную только для чтения копию данных из всех различных OLTP-систем компании. Данные извлекаются из баз OLTP (с помощью выполнения периодических дампов данных или непре- рывного потока обновлений данных), преобразуются в удобный для анализа вид, очищаются и затем загружаются в склад. Процесс их помещения в склад известен под названием «извлечение — преобразование — загрузка» (extract — transform — load, __ETL__)
Множе- ство складов данных применяются довольно шаблонным образом, известным под названием __«схема “звезда”»__ (star schema, также моделирование с помощью измерений (dimensional modeling)). В центре схемы находится так называемая таблица фактов. Каждая строка таблицы отражает событие, произошедшее в конкретный момент времени. Обычно факты поступают в виде отдельных событий, поскольку таким образом обеспечивается максимальная гибкость дальнейшего анализа. 
Отдельные столбцы таблиц фактов представляют собой __атрибуты__, такие как цена, по которой был продан товар, и стоимость его закупки у поставщика (благодаря чему можно вычислить размер прибыли). Другие столбцы представляют собой __внешние ключи__ к другим таблицам, именуемым __таблицами измерений__ (dimension table). В то время как строка в таблице фактов соответствует событию, измерения соответствуют «кто», «что», «где», «когда», «как» и «почему» этого события.
Название «схема “звезда”» возникло потому, что при визуализации связей таблиц таблица фактов находится посередине, окруженная таблицами измерений, а связи с этими таблицами напоминают лучи звезды.
Разновидность этого шаблона известна под названием __«схемы “снежинка”»__. В ней измерения далее разбиваются на __подызмерения__. Например, в ней могут быть отдельные таблицы для марок и категорий товаров, а каждая строка в таблице товаров может ссылаться на марку и категорию в виде внешних ключей вместо хранения их в виде строк в самой таблице Схемы «снежинки» более нормализованы, чем схемы «звезды», но последние используются чаще, поскольку с ними удобнее работать аналитикам.

В большинстве баз данных __OLTP__ хранилище располагается __построчно__: все значения из одной строки таблицы хранятся рядом друг с другом. Документо­ориентированные БД устроены аналогично: весь документ обычно хранится в виде непрерывной последовательности байтов. Идея __столбцовых хранилищ__ проста: нужно хранить рядом значения не из одной строки, а из одного столбца. Если каждый столбец хранится в отдельном файле, то запросу требуется только прочитать и выполнить синтаксический разбор не- обходимых запросов столбцов, что может сэкономить массу усилий. 
Помимо загрузки с диска только тех столбцов, которые нужно для запроса, можно еще более снизить требования к пропускной способности диска, __сжав данные__. Столбцовое хранилище часто очень хорошо поддается сжатию, поскольку __значения в них часто повторяются__. Сжатие лучше всего работает для __первого ключа сортировки__. Второй и третий ключи будут сильнее перемешаны, а следовательно, в них не будет таких длинных серий повторяющихся значений. Расположенные далее в списке сортировки столбцы окажутся, по сути, в случайном порядке, так что вряд ли будут сжаты очень хорошо. Но и сортировка первых нескольких столбцов приносит немалые плоды.
Наличие __нескольких порядков сортировки__ в столбцовом хранилище слегка напо- минает ряд вторичных индексов в построчном хранилище. Но важное различие состоит в том, что в построчном хранилище каждая строка хранится в одном месте (в неупорядоченном файле или кластеризованном индексе) и вторичные индексы лишь содержат указатели на соответствующие строки. В столбцовом хранилище же обычно нет никаких указателей на данные, только содержащие значения столбцы.
Столбцовое хранение, сжатие и сортировка служат для __ускорения выполнения запросов для чтения__. Однако у них есть __недостаток в виде __усложнения операций записи.__Хорошее решение этой проблемы - LSM-деревья. Все записывается сначала в хранилище в оперативной памяти, где данные до- бавляются в отсортированную структуру и подготавливаются к записи на диск. Не имеет значения, является хранилище в оперативной памяти столбцовым или построчным. При накоплении достаточного количества записываемых данных они объединяются с файлами столбцов на диске и записываются блоками в новые файлы. По сути, именно так и функционирует Vertica.

Другой нюанс складов данных: __материализованные сводные показатели__. Как уже обсуждалось ранее, запросы к складам часто включают функции агрегирования, такие как COUNT, SUM, AVG, MIN или MAX в языке SQL. Если во множестве различных запросов используются одни и те же функции агрегирования, то было бы расточительством каждый раз «перемалывать» необ- работанные данные. Почему бы не кэшировать часть сводных показателей, при- меняемых в запросах чаще всего? Один из способов создания подобного кэша — __материализованное представление__ (materialized view). В реляционной модели данных оно часто описывается анало- гично обычному (виртуальному) представлению: напоминающий таблицу объект, чье содержимое является результатом какого-то запроса. Различие состоит в том, что материализованное представление является фактической копией результатов запроса, записанной на диск, в то время как виртуальное представление — просто сокращенная форма записи для написания запросов. При чтении из виртуального представления движок SQL динамически разворачивает его в лежащий в его основе запрос и затем выполняет этот развернутый запрос.
Распространенный особый случай материализованного представления — __куб данных (data cube), или OLAP-куб (OLAP cube)__. Он представляет собой сетку сводных показателей, сгруппированных по различным измерениям.
Представьте, что у каждого факта имеются внешние ключи только к двум таблицам измерений — например, __дата и товар__. Можно нарисовать двумерную таблицу: с товарами по одной оси координат и датами — по другой. Каждая ячейка содержит сводный показатель (например, SUM) атрибута (например, __цена__) всех фактов с таким сочетанием даты и товара. Затем можно применить ту же агрегирующую функцию вдоль каждой строки или столбца и получить итоги, сокращенные на одно измерение (продажи по товарам независимо от даты или продажи по датам неза- висимо от товара). 
__Преимущество__ материализованных кубов состоит в том, что определенные за- просы будут выполняться __очень быстро__, поскольку данные для них были, по сути, заранее вычислены. Например, если вам нужно узнать общий объем про- даж за вчера по каждому магазину, то достаточно просто посмотреть на итоги по соответствующему измерению — нет никакой необходимости просматривать миллионы строк. __Недостаток__ же заключается в том, что кубы данных __не имеют такой же гибкости, как запросы__ к исходным данным. Например, тут нельзя подсчитать, какая доля продаж приходится на товары, стоящие менее $100, поскольку цена не является одним из измерений. Большинство складов данных поэтому стараются хранить как можно больше исходных данных и __использовать__ сводные показатели, такие как __кубы данных__, только в качестве __ускорителя__ производительности для __определенных__ запросов.

#ACID
__Атомарность__ в ACID описывает происходящее при сбое (например, фатальном сбое процесса, разрыве сетевого соединения, переполнении диска или нарушении одного из ограничений целостности) в процессе выполнения клиентом нескольких операций записи, в момент, когда выполнена лишь их часть. Если операции записи сгруппированы в атомарную транзакцию и ее __не удается завершить__ (зафиксировать изменения) из-за сбоя, то она __прерывается__ (abort) и базе данных приходится от- бросить или __откатить все уже выполненные в рамках этой транзакции операции записи__. При возникновении ошибки во время выполнения нескольких изменений без ато- марности было бы сложно понять, какие из них вступили в действие. Приложение способно попытаться выполнить их снова, но здесь возникает риск выполнения одних и тех же изменений дважды; это может привести к дублированию или к ошибкам в них.

Идея __согласованности__ в смысле ACID состоит в том, что определенные утверждения относительно данных (инварианты) должны всегда оставаться справедливыми — например, в системе бухгалтерского учета кредит всегда должен сходиться с дебетом по всем счетам. Если транзакция начинается при допустимом (в соответствии с этими инвариантами) состоянии базы данных и любые производимые во время транзакции операции записи сохраняют это свойство, то можно быть уверенными, что система всегда будет удовлетворять инвариантам. Согласованность полагается на свойства атомарности и изоляции базы данных, чтобы обеспечить согласованность, но __не на одну только базу данных__. Следовательно, __букве C на самом деле не место в аббревиатуре ACID__.

__Изоляция__ в смысле ACID означает, что конкурентно выполняемые транзакции изолированы друг от друга — они не могут помешать друг другу. Классические учеб- ники по базам данных понимают под изоляцией сериализуемость (serializability). То есть каждая транзакция выполняется так, будто она единственная во всей базе. БД гарантирует, что результат фиксации транзакций такой же, как если бы они выполнялись последовательно (serially, одна за другой), хотя в реальности они могут выполняться конкурентно [10].

Задача СУБД — предоставить надежное место для хранения данных. __Сохраня­емость__ (durability) — обязательство базы не терять записанных (успешно зафик- сированных) транзакций данных, даже в случае сбоя аппаратного обеспечения или фатального сбоя самой БД.
В одноузловой базе сохраняемость обычно означает запись данных на энергонезависимый носитель информации, например жесткий диск или SSD. Она обычно подразумевает также наличие журнала упреждающей записи или чего-то в этом роде, обеспечивающего возможность восстановления в случае повреждения структуры данных на диске. В реплицируемой БД сохраняемость может означать, что данные были успешно скопированы на некоторое количество узлов. Для обеспечения гарантии сохраняемости база должна дожидаться завершения этих операций записи или репликаций, прежде чем сообщать об успешной фиксации транзакции. 
Как обсуждалось ранее __абсолютная надежность недостижима__: если все жесткие диски и резервные копии будут уничтожены одновременно, то база данных, безусловно, не сможет никак вас спасти. На практике не существует ни одного метода, дающего абсолютную гарантию сохраняемости данных. Существуют только различные методики снижения рисков, включая запись на диск, репликацию на удаленные машины и выполнение резервных копий, которые можно и нужно комбинировать. Как и всегда, к любым теоретическим «гарантиям» следует относиться с долей здорового скептицизма.

При __двухфазной блокировке__ записывающие транзакции __блокируют__ не просто другие __записывающие__ транзакции, но __и читающие__ и наоборот. 
При __изоляции снимков состояния__ — читающие транзакции никогда не блокируют записывающие, а записывающие никогда не блокируют читающие. 
__Двухфазная блокировка__, обеспечивая __сериализуемость__, __защищает от всех__ обсуждавшихся выше __состояний гонки__, включая потери обновлений и асимметрию записи.
• __Сериализуемость__ — свойство изоляции транзакций, при котором каждая транз­акция может читать и записывать __несколько__ объектов (строки, документы, за- писи). Это гарантирует, что транзакции ведут себя так же, как если бы выполнялись в некой последовательности (каждая из них завершается до начала следующей). Отличие такого порядка запросов от порядка, в котором транзакции фактически выполнялись, считается нормальным.

#Реализация двухфазной блокировки# 
Чтобы реализовать блокировку читающих и записывающих транзакций, на каждом объекте в базе данных имеется блокировка, которая может находиться или в разде- ляемом (shared mode), или в монопольном режиме (exclusive mode). Эти блокировки применяются следующим образом. 
* Перед чтением объекта транзакция должна сначала установить блокировку в разделяемом режиме. Допускается удержание блокировки в разделяемом режиме несколькими транзакциями одновременно, но если какая-либо еще транзакция уже установила монопольную блокировку на объект, то этой транзакции следует подождать снятия монопольной блокировки. 
* Перед записью объекта транзакция сначала должна установить блокировку в монопольном режиме. Другим транзакциям не разрешается удерживать блокировку одновременно с ней (ни в разделяемом, ни в монопольном режиме), так что если на объект уже установлена блокировка, то ей придется подождать. 
* При чтении объекта с последующей записью транзакция может повысить уровень блокировки с разделяемой до монопольной. Такое повышение уровня блокировки выполняется аналогично непосредственной установке монополь- ной блокировки. 
* После установки блокировки транзакция должна продолжать удерживать ее вплоть до завершения (фиксации или прерывания). Отсюда и название «двух- фазная»: __первая фаза__ (во время выполнения транзакции) представляет собой __получение блокировок__, а __вторая__ (в конце выполнения транзакции) — их __освобождение__.
Из-за столь большого количества блокировок часто встречается ситуация, когда транзакция A ждет снятия блокировки транзакции B и наоборот. Такая ситуация называется __взаимной блокировкой (deadlock)__. База данных автоматически обнару- живает взаимные блокировки между транзакциями и прерывает одну из них, чтобы остальные могли продолжить работу. Приложению приходится повторно выпол- нять прерванную транзакцию.
Главный __недостаток__ двухфазной блокировки и причина, по которой ее не исполь- зовали повсеместно с 1970-х годов, — ее __низкая производительность__: пропускная способность по транзакциям и время отклика запросов значительно хуже при двухфазной блокировке, чем при слабой изоляции.

__Пессимистическое и оптимистическое управление конкурентным доступом__
__Последовательное выполнение__ в некотором смысле предельно __пессимистично__: по сути, оно эквивалентно тому, что каждая транзакция удерживает __монопольную блокировку на всю базу данных (или всю секцию базы) на все время выполнения транзакции__. Пессимистичность компенсируется максимальным ускорением отдельных транзакций, поэтому блокировки приходится удерживать только очень короткий период времени. 
Напротив, __сериализуемая изоляция снимков состояния__ представляет собой __оптимистический__ метод управления конкурентным доступом. «Оптимистический» в этом контексте означает следующее: __вместо блокировки__ в случае потенциально опасных действий транзакции просто __продолжают выполняться__ в надежде, что все будет хорошо. При __фиксации транзакции__ база данных проверяет, не __случилось ли чего-то плохого__ (например, не была ли нарушена изоляция). Если да, то __транзакция прерывается__ и ее выполнение приходится __повторять__ еще раз. Допускается фиксация только выполненных сериализуемым образом транзакций.
Если система уже и так близка к своей максимальной пропускной способности, то нагрузка в виде дополнительных повторно выполняемых транзакций может ухудшить производительность. Однако если резерв возможностей системы достаточно велик, а конкуренция между транзакциями не слишком высока, то методы оптимистического управления конкурентным доступом демонстрируют более высокую производительность, чем пессимистического. Конкуренцию можно понизить с помощью коммутативных атомарных операций: например, при конкурентном увеличении значения счет- чика несколькими транзакциями порядок выполнения операций не играет роли (конечно, если не производится чтение счетчика в той же транзакции), так что они не конфликтуют.

Подытоживая, только изоляция уровня сериализуемых транзакций предотвращает все проблемы состояний "гонки". Выделяются три различных подхода к реализации сериализуемых транзакций. 
* По-настоящему последовательное выполнение транзакций. Если вы можете сделать отдельные транзакции очень быстрыми, причем количество транзакций, обрабатываемых за единицу времени на одном ядре CPU, достаточно невелико, то для обработки этот вариант окажется простым и эффективным. 
* Двухфазная блокировка. На протяжении десятилетий она была стандартным способом обеспечения сериализуемости, но многие приложения стараются ее не использовать из-за плохих показателей производительности. 
* Сериализуемая изоляция снимков состояния (SSI). Довольно свежий алгоритм, лишенный практически всех недостатков предыдущих подходов. В нем ис- пользуется оптимистический подход, благодаря чему транзакции выполняются без блокировок. Перед фиксацией транзакции выполняется проверка, и если выполнение было несериализуемым, то транзакция прерывается без фиксации.

##Распределенные системы
Основные проблемы, возникающие в распределенных системах:
1. Вероятность потери или задержки на произвольное время отправленного по сети пакета. Аналогично может потеряться или задержаться ответ, так что при его отсутствии вы не будете знать, дошло ли ваше сообщение до адресата. 
2. Часы узла могут оказаться существенно рассинхронизированными с другими узлами (несмотря на все ваши старания по настройке NTP), неожиданно пере- скакивать вперед или назад во времени, и полагаться на них опасно, ведь вы не знаете их интервала погрешности. 
3. Выполнение процесса может быть приостановлено в любой момент на суще- ственное время (например, вследствие всеобъемлющей паузы на сборку мусо- ра), его могут объявить неработающим другие узлы, причем он способен потом возобновить работу, понятия не имея, что останавливался. 
Возможность подобных частичных отказов (partial failures) — определяющая характеристика распределенных систем. При всяком действии программного обеспечения, вовлекающем использование других узлов, существует вероятность сбоя, или замедления работы, или того, что узел вообще перестанет отвечать (и по- степенно время ожидания истечет). В распределенных системах мы пытаемся сделать ПО устойчивым к частичным отказам, чтобы система в целом продолжала функционировать даже при отказе некоторых ее элементов.
Лучший способ построить отказоустойчивую систему — создать некоторые __общие абстракции с полезными гарантиями__, реализовать их один раз, а затем позволить приложениям полагаться на эти гарантии.
Одной из самых важных абстракций для распределенных систем является __консенсус__ — согласованность между всеми узлами по какому-то вопросу. Надежное обеспечение консенсуса, несмотря на сетевые сбои и отказы процессов, — удивительно сложная задача.

__Линеаризуемость — гарантия возврата при чтении и записи реестра__ (отдельного объекта): после того как новое значение записано или прочитано, все последующие операции чтения выдают то значение, которое было записано, до тех пор пока оно не будет перезаписано снова.
Линеаризуемость нужна, например, при __ограничениях и гарантии уникальности__. Ограничения уникальности широко распространены в базах данных: например, имя пользователя или адрес электронной почты должны однозначно его иденти- фицировать, а в сервисе хранения файлов не может быть двух файлов с одинако- выми путем и именем. Чтобы принудительно применить данное ограничение при записи данных, нужна линеаризуемость (например, когда два человека пытаются конкурентно создать пользователя или файл с одинаковыми именами, одна из этих операций завершится с ошибкой).
__Теорема CAP__
* Если приложение __требует линеаризуемости__, а некоторые реплики не имеют доступа к другим репликам из-за __проблем в сети__, то эти реплики не могут обрабатывать запросы, пока они отключены: они должны либо ждать, пока связь будет восстановлена, либо возвращать ошибку (в любом случае они становятся __недоступными__).
* Если приложение __не требует линеаризуемости__, то оно может быть записано таким образом, чтобы каждая реплика могла обрабатывать запросы независимо, даже будучи __отключенной от других реплик__ (например, в конфигурации с несколькими ведущими узлами). Тогда приложение __может оставаться доступным__ даже в случае проблем с сетью, но его поведение __нелинеаризуемо__.
При возникновении сбоя в сети приходится выбирать либо линеаризуемость, либо доступность. Таким образом, удачным вариантом расшифровки CAP будет __either Consistent or Available when Partitioned__ — «либо последовательность, либо доступность при нарушении связности сети». В более надежных сетях этот выбор приходится делать реже, но в какой-то момент он все равно неизбежен.
Более быстрый алгоритм для линеаризуемости не существует, но более слабые модели согласованности могут оказаться гораздо быстрее, и этот компромисс важен для систем, чувствительных к задержкам.
__Сохранение последовательности__ это важная фундаментальная идея.
* Главная цель ведущего узла в репликации с одним таким узлом — определить последовательность операций записи в журнале репли- кации, то есть порядок, в котором ведомые узлы будут выполнять эти операции. При отсутствии единого ведущего узла возможны конфликты из-за конкурент- ных операций
* Сериализуемость заключается в обеспечении того, чтобы транзакции вели себя так, как если бы они выполнялись в некой последователь- ности. Этого можно достичь, действительно выполняя транзакции в таком порядке либо допуская конкурентное выполнение, но предотвращая конфликты сериализации (путем блокировки или прерывания). 
* Использование временных меток и часов в распределенных системах - еще одна попытка внести порядок в этот беспорядочный мир, например, чтобы определить, какая из двух записей произошла позже.
Есть несколько причин, почему значение сохранения правильной последовательности продолжает расти, и одна из них такова: это помогает сохранить __причинность__.
__Причинность__ требует расположения событий __в определенном порядке__: причина должна идти раньше результата; сообщение отправляется до того, как было полу- чено; вопрос приходит перед ответом. Как и в реальной жизни, одно вытекает из другого: один узел читает некие данные, а затем записывает результат, другой узел читает записанный результат и, в свою очередь, что-то пишет и т. д. Эти цепочки причинно-зависимых операций определяют причинно-следственный порядок в системе — что перед чем произошло. В случае подчинения системы последовательности, соответствующей причинно- следственным связям, говорят, что она причинно-согласованна. Например, изоляция снимков состояния обеспечивает причинную согласованность: если при чтении БД вы видите какую-то часть данных, то должны иметь возможность видеть и любые данные, которые причинно предшествуют этим (при условии, что они не были удалены ранее).
__Причинно-следственное упорядочение — это не полное упорядочение__
Линеаризуемость гарантирует сохранение причинности, поэтому линеаризуемые системы столь понятны и привлекательны. Однако, как обсуждалось ранее, сделать систему линеаризуемой обычно означает снизить ее производительность и доступность, особенно если ей свойственны значительные сетевые задержки (например, при условии, что она географически распределена). Как следствие, некоторые рас- пределенные информационные системы отказались от линеаризуемости — это позволяет им обеспечивать более высокую производительность, хотя и может за- труднять работу с ними. Хорошей новостью является то, что здесь можно найти золотую середину. Линеари- зуемость не единственный способ сохранения причинности, есть и другие. Система может быть причинно-последовательной без падения производительности, воз- можном при линеаризации (в частности, таких систем не касается теорема CAP). В сущности, причинно-следственная согласованность — это наиболее сильная модель согласованности, которая не замедляет работу системы из-за сетевых задержек и остается доступной при сбоях сети. __Во многих случаях системы, на первый взгляд требующие линеаризуемости, в действительности требуют только причинно-следственную согласованность, которая может быть реализована более эффективно.__
Проблема заключается в следующем: полное упорядочение операций возникает __только после того, как были собраны все операции__. Если другой узел сгенери- ровал некоторые операции, но вы еще не знаете, что они собой представляют, то не можете построить полную последовательность операций: может появиться не- обходимость вставить неизвестные операции от другого узла в произвольные места общей последовательности.
__Рассылка общей последовательности__ обычно описывается как протокол обмена сообщениями между узлами. Неформально это требует, чтобы всегда выполнялись два следующих требования безопасности. 
1. Надежная доставка. Ни одно сообщение не должно быть потеряно. Если со- общение доставляется одному узлу, то оно доставляется всем узлам. 
2. Полностью упорядоченная доставка. Сообщения доставляются во все узлы в одном и том же порядке.
Рассылка общей последовательности — именно то, что нужно для репликации базы данных: если каждое сообщение представляет собой запись в базу и каждая реплика обрабатывает одни и те же записи в той же последовательности, то ре- плики будут оставаться согласованными между собой (кроме временных задержек репликации). Этот принцип известен как репликация конечных автоматов.

__Консенсус__ является одной из важнейших и фундаментальных проблем распределенных вычислений. На первый взгляд все кажется легким: говоря простыми словами, цель состоит в том, чтобы заставить несколько узлов согласовать некие объекты. Вы могли подумать: это не должно быть слишком сложно. К сожалению, многие системы были разрушены, поскольку их создатели исходили из ошибочного убеждения, что данную проблему легко решить.
__Двухфазная фиксация (2PC)__ является алгоритмом для атомарной фиксации транзакций в случае нескольких узлов. Другими словами, она гарантирует, что все узлы либо зафиксировали транзакцию, либо прервали ее. Это классический алгоритм для распределенных баз данных. Алгоритм 2PC применяется внутри отдель- ных БД, а также доступен для приложений в виде XA-транзакций (которые, например, поддерживаются Java Transaction API) или через WS-AtomicTransaction для веб-сервисов SOAP.
Транзакция 2PC начинается с того, что приложение, как обычно, выполняет чтение и запись данных в нескольких узлах БД. Эти узлы называют участниками транз­акции. Когда приложение готово к фиксации, координатор начинает этап 1: он отправляет запрос на подготовку каждому из узлов, спрашивая их, могут ли они выполнить фиксацию. Затем координатор отслеживает ответы участников. 
1. Если все участники ответили «да», показав, что они готовы к фиксации, то на этапе 2 координатор отправляет запрос фиксации и выполняется фиксация.
2. Если один из участников ответил «нет», то на этапе 2 координатор отправляет всем узлам запрос прерывания.
Таким образом, в данном протоколе есть две критические точки невозврата: 
1. Когда участник отвечает «да», он обещает, что гарантированно сможет позднее завершить транзакцию (хотя координатор все еще может отказаться от нее)
2. Как только координатор принимает решение, оно является бесповоротным.
Двухфазную фиксацию называют __блокирующим протоколом атомарной фиксации__ из-за того, что 2PC способна зависнуть в ожидании восстановления координатора.
__X/Open XA (сокращенное eXtended Architecture)__ — это стандарт для реализа- ции двухфазной фиксации в гетерогенных технологиях. Он был внедрен в 1991 году и с тех пор широко применяется: XA поддерживается во многих тра- диционных реляционных базах данных (включая PostgreSQL, MySQL, DB2, SQL Server и Oracle) и брокерах сообщений (таких как ActiveMQ, HornetQ, MSMQ и IBM MQ).
Проблема сбоя координатора в том, что при ис- пользовании двухфазной фиксации транзакция должна сохранять блокировки все время, пока находится в состоянии неопределенности. При выходе координатора из строя и повторном включении через 20 минут эти блокировки будут удержи- ваться в течение 20 минут. Если по какой-то причине журнал координатора будет полностью потерян, то блокировки будут сохраняться вечно или по крайней мере до тех пор, пока администратор не решит проблему вручную. Пока сохраняются блокировки, никакая другая транзакция не может изменять эти строки. В зависимости от базы данных они могут быть заблокированы для чтения и другими транзакциями. Таким образом, последние не могут просто продолжать свою работу — если им нужен доступ к тем же данным, то они будут заблокированы. Это может привести к тому, что большая часть приложения станет недоступной до тех пор, пока не будет устранена неопределенность.

Если узел только один или в случае, когда только один узел имеет возможность принимать решения, проблемы согласованности решаются. Именно это происходит в базе данных с одним ведущим узлом: на него возложены полномочия по принятию решений. Поэтому такие базы могут предоставлять линеаризуемые операции, ограничения уникальности, полностью упорядоченный журнал репликации и др. Однако если этот единственный ведущий узел выходит из строя или становится недоступным вследствие разрыва сетевого соединения, то такая система становится неработоспособной. Существует три способа выхода из данной ситуации. 
1. Дождаться, пока ведущий узел восстановится, и согласиться с тем, что все это время система будет заблокирована. Многие координаторы XA/JTA-транзакций выбирают именно такой вариант. Данный метод не полностью решает задачу консенсуса, поскольку не удовлетворяет требованию завершения: если ведущий узел не восстановится, то система может оказаться заблокированной навсегда. 
2. Решить проблему вручную — пускай человек выберет новый ведущий узел и перенастроит систему для его использования. Такой вариант применяется во многих реляционных базах данных. Это своего рода консенсус методом «вме- шательства высшей силы»: решение принимает человек-оператор, находящийся вне компьютерной системы. Скорость разрешения проблемы ограничена той скоростью, с которой могут действовать люди, — как правило, медленнее, чем компьютеры.
3. Использовать алгоритм автоматического выбора нового ведущего узла. Такой вариант требует консенсусного алгоритма, и рекомендуется задействовать проверенный алгоритм, который правильно обрабатывает неблагоприятные сетевые условия.

Такие инструменты, как ZooKeeper, играют важную роль в обеспечении «аутсорсинга» консенсуса, обнаружении сбоев и обслуживании членства, которое могут задействовать приложения. Данная система непроста в применении, но это на- много лучше, чем пытаться разработать собственные алгоритмы для решения всех проблем, описанных в главе 8. Если однажды вы захотите решить одну из задач, которые сводятся к консенсусу, и сделать решение отказоустойчивым, то советую использовать что-то наподобие ZooKeeper. 
Однако не каждая система обязательно требует консенсуса: например, системы репликации без ведущего узла и системы с несколькими такими узлами обычно не используют глобальный консенсус. Конфликты, возникающие в подобных си- стемах, являются следствием отсутствия консенсуса между разными ведущими узлами. Но, может быть, это нормально: вероятно, нужно лишь научиться обходиться без линеаризуемости и лучше работать со структурами данных, история которых имеет ветвления с по- следующим слиянием версий.