Совместная вероятность (по И) :
P (A, B) = P (A) × P (B | A).
Вероятность одного ИЛИ другого события чуть сложнее, так как события могут быть взаимоисключающими, а могут и не быть. 
Вероятность по ИЛИ : P (A ИЛИ B) = P (A) + P (B) – P (A) × P (B | A).

Биномиальное распределение ис пользуется для подсчета числа успешных исходов, когда мы знаем число попыток и вероятность успеха. Приставка «би» означает, что возможных исходов два: событие происходит или нет. Если возможных исходов более двух, распределение называется мультиномиальным. Биномиальное распределение описывает вероятности:
дважды выбросить орла за три броска монеты; 
купить миллион лотерейных билетов и выиграть хотя бы один раз; 
выбросить двадцать меньше трех раз за 10 бросков двадцатигранной кости.
Любая задача, состоящая в определении вероятности k исходов за n испытаний, где вероятность исхода равна p, легко решается с использованием биномиального распределения:
B(k;n,p)	= ( n k )*p^k*(1-p)	^n-k.

Плотность вероятности :
Beta(p;α,β)  где р обозначает вероятность события, α показывает, сколько раз произошло интересующее нас событие, β — сколько раз оно не произошло

P(предположения | данные)  - Апостериорная вероятность
P(данные | предположения) - Правдоподобие
P(предположения) - Априорная вероятность
P(данные) - Нормализация Апостериорной вероятности

Теорема Байеса: P(предположения | данные)  = P(данные | предположения) * P(предположения) / P(данные)  (1)

Когда вероятность имеющихся данных P(данные) уменьшается, апостериорная вероятность P(предположения | данные)  увеличивается. Это связано с тем, что по мере того как наблюдаемые нами данные становятся все более маловероятными, то обычно маловероятное объяснение лучше объясняет событие.

Если P(данные) отсутствует можно оценить альтернативные гипотезы через отношения ненормализованных вероятностей (формула 1 без деления на P(данные)).

Среднее значение - μ 
Стандартное отклонение σ  = sqrt (1/n * sum(1,n, (x(i)-μ )^2)

Нормальное распределение — это непрерывное распределение вероятностей (например, бета-распределение в главе 5), которое наилучшим образом описывает силу возможных убеждений в значении неопределенного измерения, учитывая известное среднее значение и стандартное отклонение. Оно принимает значения μ и σ (среднее значение и стандартное отклонение соответственно) в качестве двух параметров.

Таблица 12.1. Области под кривой для различных средних значений 
Расстояние от среднего значения 		Вероятность 
	σ 									68 % 
	2σ 									95 % 
	3σ 									99,7 %

Ключевой момент: только когда вы ничего не знаете о данных, кроме их среднего значения и дисперсии, безопасно предполагать нормальное распределение.

PDF - функция плотности вероятности. PDF — это функция, которая принимает значение и возвращает вероятность этого значения. Наиболее распространенное математическое использование PDF — это интегрирование для определения вероятностей, связанных с различными диапазонами. 
CDF - кумулятивная функция распределения. CDF принимает значение и возвращает вероятность получения этого или меньшего значения.
Например, можно легко определить медиану по визуализации CDF. Просто проведите линию на графике CDF от точки, где совокупная вероятность равна 0,5; это означает, что 50 % значений располагается ниже этой точки, а 50 % — выше. 
С помощью CDF можно оценить доверительные интервалы.  Допустим, нужно узнать диапазон, который охватывает 80 % возможных значений для искомого параметра. Решим эту задачу, комбинируя предыдущие подходы: рисуем линии на оси Y от 0,1 до 0,9, чтобы покрыть 80 %, а затем смотрим, где на оси X они пересекаются с графиком CDF.
Квантильная функция - инверсия CDF (график выглядит, как график CDF повернутый на 90 градусов).

«Слабая» априорная вероятность означает, что она будет легко переопределена фактическими данными, поскольку мы соберем еще больше информации. Более сильная априорная вероятность,потребовала бы больше доказательств для изменения. Самое главное — даже если априорная вероятность неверна, она в конечном итоге будет отменена данными, когда вы соберете больше наблюдений.
Вычисление плотности вероятности при наличии априорных данных:
Beta(α_апостериорная вероятность, β_апостериорная вероятность) = = Beta(α_правдоподобность + α_априорная вероятность, β_правдоподобность + β_априорная вероятность).


Другая формулировка теоремы Байеса:
P (H | D) = P (D | H)  *  P (H)  /  P (D), 

где
P (H | D) — апостериорная вероятность, которая указывает, как сильно мы должны верить в гипотезу, учитывая данные; 
P (H) — априорное убеждение, или вероятность гипотезы до просмотра данных; 
P (D | H) — правдоподобность получения существующих данных в слу- чае, если бы наша гипотеза была верной.

Последняя часть, P (D), является вероятностью данных, наблюдаемых не- зависимо от гипотезы. Эта часть нужна, чтобы убедиться, что апостериор- ная вероятность правильно размещена где-то между 0 и 1. Если у нас есть все эти фрагменты информации, мы можем точно рассчитать, насколько сильно следует верить в гипотезу в условиях наблюдаемых данных. Зачастую P (D) очень трудно определить. Во многих случаях не очевидно, как можно выяснить вероятность наших данных. P (D) также совершенно не нужна, если все, что нас волнует, — это сравнение относи- тельной силы двух разных гипотез.Мы можем использовать это для сравнения двух гипотез, исследовав соотношение априорного убеждения, умноженное на вероятность для каждой гипотезы, и применив формулу отношения апо- стериорных вероятностей: 
P (H1)  * P (D | H1)  /  P (H2)  * P (D | H2) 

Если отношение равно 2, то H1 объясняет наблюда- емые данные в два раза лучше, чем H2.
Если априорные убеждения одинаковы, т.е. P (H1) = P (H2), мы получаем коэффициент Байеса:
P (D | H1)  /  P (D | H2) 
Если априорные убеждения НЕ одинаковы, можно вычислить АПРИОРНЫЙ ШАНС: 
О(Н1) = P (H1)  /  P (H2).
Это полезно, потому что позволяет заметить, насколько сильно (или слабо) мы верим в гипотезу, которую проверяем. Когда это число больше 1, это означает, что априор- ные шансы подтверждают гипотезу, а когда оно меньше 1, это означает, что они противоречат гипотезе. Например, O (H1) = 100 означает, что без какой-либо другой информации мы считаем, что H1 в 100 раз более вероятна, чем альтернативная гипотеза. С другой стороны, когда O (H1) = = 1/100, альтернативная гипотеза в 100 раз более вероятна, чем наша.

АПОСТЕРИОРНЫЙ ШАНС = О(Н1) * P (D | H1)  /  P (D | H2) 

Таблица 16.1. Рекомендации по оценке апостериорных шансов 
Апостериорные шансы 				Сила доказательств 
	1 к 3 					Интересно, но ничего неопровержимого 
	3 к 20 					Похоже, мы к чему-то движемся 
	20 к 150 					Сильные доказательства в пользу H2 
	> 1 к 150					Неопровержимые доказательства